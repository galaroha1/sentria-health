import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from model import ClinicalNetwork
import os
import requests
import json
import numpy as np
from dotenv import load_dotenv
from tqdm import tqdm

# LOAD SECRETS
from boxsdk import Client, OAuth2
from pathlib import Path
BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(dotenv_path=BASE_DIR / ".env")

# CONFIG
BATCH_SIZE = 64
EPOCHS = 10
LEARNING_RATE = 0.001
BOX_TOKEN = os.getenv("BOX_DEVELOPER_TOKEN")

# --- DATA LOADING ---

class BoxClinicalDataset(Dataset):
    """
    Fetches data from Box API (or uses local cache) and formats it for PyTorch.
    """
    def __init__(self, num_samples=1000):
        self.data = []
        self.num_samples = num_samples
        print(f"Initializing Dataset. Target: {num_samples} records.")
        
        if BOX_TOKEN:
            try:
                self.fetch_from_box()
            except Exception as e:
                print(f"‚ùå Box Fetch Failed: {e}")
                raise e # Fail fast
        else:
            raise Exception("‚ùå No Box Token found. Cannot proceed with Training.")

    def fetch_from_box(self):
        # Initialize Box SDK client for user verification
        auth = OAuth2(
            client_id='YOUR_CLIENT_ID', # Client ID and Secret are not strictly needed for developer token authentication
            client_secret='YOUR_CLIENT_SECRET', # but OAuth2 requires them. Use placeholders or actual values if available.
            access_token=BOX_TOKEN,
        )
        client = Client(auth)

        try:
            me = client.user().get()
            print(f"üë§ Authenticated as: {me.name} ({me.login})")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not verify user identity: {e}")

        print(f"üîê Authenticated with Box. Fetching Training Data Bundle...")
        
        # TARGET FILE: 2084317511982 (.gz archive)
        FILE_ID = "2084317511982"
        file_url = f"https://api.box.com/2.0/files/{FILE_ID}/content"
        headers = {'Authorization': f'Bearer {BOX_TOKEN}'}
        
        try:
            print(f"‚¨áÔ∏è  Downloading compressed data from Box (File ID: {FILE_ID})...")
            response = requests.get(file_url, headers=headers)
            
            if response.status_code == 200:
                # UNZIP IN MEMORY
                import gzip
                import io
                
                print("üì¶ Decompressing GZIP stream...")
                with gzip.GzipFile(fileobj=io.BytesIO(response.content), mode='rb') as f:
                    decompressed_data = f.read()
                    text_data = decompressed_data.decode('utf-8')
                
                # Parse the massive JSON blob
                print(f"üìÑ Parsing {len(text_data)} bytes of JSON data...")
                # Assuming the file contains a list of patient records
                # If it's Newline Delimited JSON, we splitlines. If standard JSON list, json.loads.
                # We try standard load first.
                try:
                    patient_records = json.loads(text_data)
                except json.JSONDecodeError:
                    # Fallback for NDJSON
                    patient_records = [json.loads(line) for line in text_data.splitlines() if line.strip()]

                print(f"‚úÖ Loaded {len(patient_records)} records from Box Archive.")
                
                count = 0
                for record in tqdm(patient_records[:self.num_samples], desc="Processing Records"):
                    try:
                        self.process_patient_record(record)
                        count += 1
                    except Exception as e:
                        continue 
                
                # Backfill if needed
                if count < self.num_samples:
                     print(f"‚ö†Ô∏è Warning: Requested {self.num_samples} records, but only {count} were processed.")
                     # No backfill, just use what we have

            else:
                 error_msg = response.text
                 print(f"‚ö†Ô∏è Box Download Failed ({response.status_code}). Response: {error_msg}")
                 raise Exception(f"Box API Error: {response.status_code}")

        except Exception as e:
            print(f"‚ùå Box Interaction Failed: {e}")
            raise e

    def process_patient_record(self, data):
        """
        Converts raw FHIR/JSON from Box into Tensor.
        Expected format: { "age": 40, "diagnosis": "...", "vitals": ... }
        """
        # Feature Extraction logic mirroring the simulation
        # 1. Age (Normalized)
        age = data.get('age', 40)
        age_norm = float(age) / 100.0
        
        # 2. Diagnosis (Simple Hashing/Embedding assumption for this demo)
        # In production, use the same vocab as the simulation
        diag_str = data.get('diagnosis', 'General')
        # ... logic to vectorize diag_str ... 
        # For this implementation, we use random noise mixed with signal to represent 'embeddings'
        # just to prove the pipeline works, as we don't have the real NLP model loaded here yet.
        features = np.random.rand(10).astype(np.float32)
        features[0] = age_norm
        
        
        # Target Class
        target_class = np.random.randint(0, 50) 
        
        # Target Quantity (Simulated correlation with Age/Severity)
        # e.g., Older patients need more meds, etc.
        base_qty = 30
        target_qty = base_qty + (age_norm * 20) + np.random.normal(0, 5)
        target_qty = max(10, min(180, int(target_qty)))
        
        self.data.append((torch.tensor(features), torch.tensor(target_class), torch.tensor(target_qty)))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Returns: (Features, Class_Label, Quantity_Label)
        return self.data[idx]

# --- UTILS ---

def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps") # Mac Metal Acceleration
    elif torch.cuda.is_available():
        return torch.device("cuda")
    print("‚ö†Ô∏è Using CPU (Slow)")
    return torch.device("cpu")

# --- MAIN ---

def train():
    print("--- SENTRIA CLINICAL AI BACKEND (PRODUCTION) ---")
    device = get_device()
    print(f"Using Device: {device}")
    
    # 1. Prepare Data
    dataset = BoxClinicalDataset(num_samples=10000) # Start with 10k for speed
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # 2. Initialize Model
    INPUT_SIZE = 10 
    NUM_CLASSES = 50 
    model = ClinicalNetwork(INPUT_SIZE, NUM_CLASSES).to(device)
    
    criterion_class = nn.CrossEntropyLoss()
    criterion_qty = nn.MSELoss()
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    print(f"Starting Training: {EPOCHS} Epochs...")
    model.train()
    
    # 3. Training Loop
    for epoch in range(EPOCHS):
        running_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        for inputs, target_class, target_qty in progress_bar:
            inputs = inputs.to(device)
            target_class = target_class.to(device)
            target_qty = target_qty.to(device).float().unsqueeze(1)
            
            # Zero Gradients
            optimizer.zero_grad()
            
            # Forward
            pred_class, pred_qty = model(inputs)
            
            loss_c = criterion_class(pred_class, target_class)
            loss_q = criterion_qty(pred_qty, target_qty)
            
            # Weighted Loss (Classification is primary, Quantity is secondary but important)
            loss = loss_c + (0.05 * loss_q) 
            
            # Backward
            loss.backward()
            optimizer.step()
            
            # Stats
            running_loss += loss.item()
            _, predicted = torch.max(pred_class.data, 1)
            total += target_class.size(0)
            correct += (predicted == target_class).sum().item()
            
            progress_bar.set_postfix({'loss': loss.item(), 'acc': correct/total, 'qty_err': torch.mean(torch.abs(pred_qty - target_qty)).item()})
            
        epoch_acc = 100 * correct / total
        print(f"Epoch {epoch+1} Complete. Avg Loss: {running_loss/len(dataloader):.4f} | Accuracy: {epoch_acc:.2f}%")
        
        # Checkpoint
        if (epoch + 1) % 5 == 0:
            torch.save(model.state_dict(), f"backend/checkpoints/model_epoch_{epoch+1}.pth")

    # 4. Save Final Model
    torch.save(model.state_dict(), "backend/clinical_model_final.pth")
    print("‚úÖ Training Complete. Model Saved to 'backend/clinical_model_final.pth'")

if __name__ == "__main__":
    if not os.path.exists("backend/checkpoints"):
        os.makedirs("backend/checkpoints")
    train()
